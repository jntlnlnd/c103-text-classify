{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def torch_fix_seed(seed=0):\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms = True\n",
    "\n",
    "\n",
    "torch_fix_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "# Word2Vecモデルのロード\n",
    "# gensimで学習済みモデルを読み込む\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"../data/word2vec_ja/jawiki.word_vectors.100d.txt\", binary=False\n",
    ")\n",
    "\n",
    "# 重みを取得\n",
    "weights = torch.FloatTensor(word2vec_model.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, weights):\n",
    "        super(SelfAttentionModel, self).__init__()\n",
    "\n",
    "        # Embedding層の作成\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, _weight=weights)\n",
    "\n",
    "        # Embedding層の重みをfreeze\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        # Self-Attention層の作成\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim, num_heads=1, batch_first=True\n",
    "        )\n",
    "\n",
    "        # 線形層の作成\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Self-Attentionの入力は(batch_size, seq_len, embedding_dim)である必要がある\n",
    "        # embeddedは(batch_size, seq_len, embedding_dim)の形状を持っていると仮定\n",
    "        attention_output, _ = self.self_attention(embedded, embedded, embedded)\n",
    "\n",
    "        attention_output_mean = (attention_output + embedded).mean(dim=1)\n",
    "\n",
    "        # Self-Attentionの出力を最終的な出力に変換するために、\n",
    "        # seq_lenの最後のベクトルを使用して線形層に渡します。\n",
    "        # attention_outputは(batch_size, seq_len, embedding_dim)の形状を持っています。\n",
    "        # ここでは、シーケンスの最後のベクトルを使用します。\n",
    "        output = self.fc(attention_output_mean)\n",
    "        return output\n",
    "\n",
    "\n",
    "# パラメータ\n",
    "vocab_size = len(word2vec_model.index_to_key)  # Word2Vecの語彙サイズ\n",
    "embedding_dim = 100\n",
    "hidden_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import MeCab\n",
    "\n",
    "mecab = MeCab.Tagger(\n",
    "    \"-O wakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
    ")\n",
    "\n",
    "\n",
    "# テキストをIDのシーケンスに変換する関数\n",
    "def text_to_sequence(text, word2vec_model):\n",
    "    return [\n",
    "        word2vec_model.key_to_index.get(word, 0) for word in mecab.parse(text).split()\n",
    "    ]\n",
    "\n",
    "\n",
    "# データセットクラス\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word2vec_model, max_length):\n",
    "        self.texts = [text_to_sequence(text, word2vec_model) for text in texts]\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # パディングの追加\n",
    "        text = self.texts[idx]\n",
    "        if len(text) < self.max_length:\n",
    "            text += [0] * (self.max_length - len(text))\n",
    "        text = text[: self.max_length]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(\n",
    "            self.labels[idx], dtype=torch.float\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imaseki/workspace/c103-text-classify/text_classify/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.635 | Train PPL: 1.886\n",
      "\t Val. Loss: 0.603 |  Val. PPL: 1.827\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.571 | Train PPL: 1.769\n",
      "\t Val. Loss: 0.582 |  Val. PPL: 1.790\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.557 | Train PPL: 1.745\n",
      "\t Val. Loss: 0.574 |  Val. PPL: 1.776\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.546 | Train PPL: 1.726\n",
      "\t Val. Loss: 0.553 |  Val. PPL: 1.739\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.525 | Train PPL: 1.690\n",
      "\t Val. Loss: 0.514 |  Val. PPL: 1.673\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.481 | Train PPL: 1.617\n",
      "\t Val. Loss: 0.453 |  Val. PPL: 1.572\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.412 | Train PPL: 1.510\n",
      "\t Val. Loss: 0.380 |  Val. PPL: 1.462\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.341 | Train PPL: 1.406\n",
      "\t Val. Loss: 0.324 |  Val. PPL: 1.382\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.290 | Train PPL: 1.336\n",
      "\t Val. Loss: 0.289 |  Val. PPL: 1.334\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.262 | Train PPL: 1.299\n",
      "\t Val. Loss: 0.265 |  Val. PPL: 1.304\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.241 | Train PPL: 1.273\n",
      "\t Val. Loss: 0.253 |  Val. PPL: 1.288\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.226 | Train PPL: 1.253\n",
      "\t Val. Loss: 0.246 |  Val. PPL: 1.279\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.220 | Train PPL: 1.246\n",
      "\t Val. Loss: 0.242 |  Val. PPL: 1.273\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.206 | Train PPL: 1.229\n",
      "\t Val. Loss: 0.239 |  Val. PPL: 1.270\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.197 | Train PPL: 1.218\n",
      "\t Val. Loss: 0.231 |  Val. PPL: 1.260\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.188 | Train PPL: 1.207\n",
      "\t Val. Loss: 0.227 |  Val. PPL: 1.255\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.186 | Train PPL: 1.205\n",
      "\t Val. Loss: 0.226 |  Val. PPL: 1.254\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.176 | Train PPL: 1.192\n",
      "\t Val. Loss: 0.222 |  Val. PPL: 1.249\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.175 | Train PPL: 1.191\n",
      "\t Val. Loss: 0.219 |  Val. PPL: 1.245\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.170 | Train PPL: 1.185\n",
      "\t Val. Loss: 0.218 |  Val. PPL: 1.243\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.165 | Train PPL: 1.180\n",
      "\t Val. Loss: 0.219 |  Val. PPL: 1.245\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.167 | Train PPL: 1.181\n",
      "\t Val. Loss: 0.213 |  Val. PPL: 1.238\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.160 | Train PPL: 1.173\n",
      "\t Val. Loss: 0.216 |  Val. PPL: 1.241\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.158 | Train PPL: 1.171\n",
      "\t Val. Loss: 0.213 |  Val. PPL: 1.238\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.153 | Train PPL: 1.166\n",
      "\t Val. Loss: 0.215 |  Val. PPL: 1.239\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.150 | Train PPL: 1.162\n",
      "\t Val. Loss: 0.215 |  Val. PPL: 1.240\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.147 | Train PPL: 1.158\n",
      "\t Val. Loss: 0.218 |  Val. PPL: 1.244\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.148 | Train PPL: 1.160\n",
      "\t Val. Loss: 0.213 |  Val. PPL: 1.237\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.143 | Train PPL: 1.154\n",
      "\t Val. Loss: 0.212 |  Val. PPL: 1.236\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.142 | Train PPL: 1.153\n",
      "\t Val. Loss: 0.211 |  Val. PPL: 1.235\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.138 | Train PPL: 1.148\n",
      "\t Val. Loss: 0.210 |  Val. PPL: 1.234\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.133 | Train PPL: 1.143\n",
      "\t Val. Loss: 0.210 |  Val. PPL: 1.233\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.135 | Train PPL: 1.145\n",
      "\t Val. Loss: 0.210 |  Val. PPL: 1.234\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.128 | Train PPL: 1.136\n",
      "\t Val. Loss: 0.217 |  Val. PPL: 1.243\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.131 | Train PPL: 1.140\n",
      "\t Val. Loss: 0.215 |  Val. PPL: 1.240\n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.123 | Train PPL: 1.131\n",
      "\t Val. Loss: 0.213 |  Val. PPL: 1.238\n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.124 | Train PPL: 1.132\n",
      "\t Val. Loss: 0.225 |  Val. PPL: 1.252\n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.117 | Train PPL: 1.124\n",
      "\t Val. Loss: 0.216 |  Val. PPL: 1.242\n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.116 | Train PPL: 1.123\n",
      "\t Val. Loss: 0.221 |  Val. PPL: 1.247\n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.116 | Train PPL: 1.122\n",
      "\t Val. Loss: 0.229 |  Val. PPL: 1.257\n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.110 | Train PPL: 1.116\n",
      "\t Val. Loss: 0.224 |  Val. PPL: 1.251\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../data/train.tsv\", sep=\"\\t\")\n",
    "valid_df = pd.read_csv(\"../data/valid.tsv\", sep=\"\\t\")\n",
    "test_df = pd.read_csv(\"../data/test.tsv\", sep=\"\\t\")\n",
    "\n",
    "# 文の最大長を決定\n",
    "max_length = max(\n",
    "    len(text_to_sequence(text, word2vec_model))\n",
    "    for text in pd.concat([train_df[\"poem\"], valid_df[\"poem\"], test_df[\"poem\"]])\n",
    ")\n",
    "\n",
    "# データセットの分割\n",
    "train_dataset = TextDataset(\n",
    "    train_df[\"poem\"], train_df[\"label\"], word2vec_model, max_length\n",
    ")\n",
    "valid_dataset = TextDataset(\n",
    "    valid_df[\"poem\"], valid_df[\"label\"], word2vec_model, max_length\n",
    ")\n",
    "test_dataset = TextDataset(\n",
    "    test_df[\"poem\"], test_df[\"label\"], word2vec_model, max_length\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2)\n",
    "\n",
    "# モデル、損失関数、最適化手法の設定\n",
    "model = SelfAttentionModel(vocab_size, embedding_dim, 1, weights)  # 出力は1次元\n",
    "loss_function = nn.BCEWithLogitsLoss()  # 2値分類のための損失関数\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)\n",
    "\n",
    "\n",
    "# 訓練関数\n",
    "def train(model, iterator, optimizer, loss_function):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for text, label in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text).squeeze(1)\n",
    "        loss = loss_function(predictions, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(iterator)\n",
    "\n",
    "\n",
    "# 評価関数\n",
    "def evaluate(model, iterator, loss_function):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, label in iterator:\n",
    "            predictions = model(text).squeeze(1)\n",
    "            loss = loss_function(predictions, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(iterator)\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = model.state_dict()\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
    "\n",
    "# 訓練ループ\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, loss_function)\n",
    "    valid_loss = evaluate(model, valid_loader, loss_function)\n",
    "    print(f\"Epoch: {epoch+1:02}\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}\")\n",
    "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):.3f}\")\n",
    "\n",
    "    early_stopping(valid_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        model.load_state_dict(early_stopping.best_model)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:  # \"_\"はラベルやターゲットを使わない場合\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 予測結果をリストに追加\n",
    "        all_predictions.extend(outputs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "pred_labels = (np.stack(all_predictions).flatten() > 0).astype(int)\n",
    "\n",
    "accuracy_score(test_df[\"label\"], pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poem</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>時は常に背後から迫り唸りを上げて眼前に流れ去る踏み止...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ああおれたちは皆眼をあけたまま空を飛ぶ夢を見てるんだ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             poem  label  pred\n",
       "3   時は常に背後から迫り唸りを上げて眼前に流れ去る踏み止...      0     0\n",
       "15     ああおれたちは皆眼をあけたまま空を飛ぶ夢を見てるんだ      0     1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 単語ロジスティック回帰で誤っていた問題を解けていたか確認\n",
    "\n",
    "test_df[\"pred\"] = pred_labels\n",
    "test_df[test_df[\"poem\"].map(lambda x: \"眼\" in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_classify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
