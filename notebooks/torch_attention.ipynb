{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def torch_fix_seed(seed=42):\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms = True\n",
    "\n",
    "\n",
    "torch_fix_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "# Word2Vecモデルのロード\n",
    "# gensimで学習済みモデルを読み込む\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"../data/word2vec_ja/jawiki.word_vectors.100d.txt\", binary=False\n",
    ")\n",
    "\n",
    "# 重みを取得\n",
    "weights = torch.FloatTensor(word2vec_model.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, weights):\n",
    "        super(SelfAttentionModel, self).__init__()\n",
    "\n",
    "        # Embedding層の作成\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, _weight=weights)\n",
    "\n",
    "        # Embedding層の重みをfreeze\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        # Self-Attention層の作成\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim, num_heads=1, batch_first=True\n",
    "        )\n",
    "\n",
    "        # 線形層の作成\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Self-Attentionの入力は(batch_size, seq_len, embedding_dim)である必要がある\n",
    "        # embeddedは(batch_size, seq_len, embedding_dim)の形状を持っていると仮定\n",
    "        attention_output, _ = self.self_attention(embedded, embedded, embedded)\n",
    "\n",
    "        attention_output_mean = (attention_output + embedded).mean(dim=1)\n",
    "\n",
    "        # Self-Attentionの出力を最終的な出力に変換するために、\n",
    "        # seq_lenの最後のベクトルを使用して線形層に渡します。\n",
    "        # attention_outputは(batch_size, seq_len, embedding_dim)の形状を持っています。\n",
    "        # ここでは、シーケンスの最後のベクトルを使用します。\n",
    "        output = self.fc(attention_output_mean)\n",
    "        return output\n",
    "\n",
    "\n",
    "# パラメータ\n",
    "vocab_size = len(word2vec_model.index_to_key)  # Word2Vecの語彙サイズ\n",
    "embedding_dim = 100\n",
    "hidden_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import MeCab\n",
    "\n",
    "mecab = MeCab.Tagger(\n",
    "    \"-O wakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
    ")\n",
    "\n",
    "\n",
    "# テキストをIDのシーケンスに変換する関数\n",
    "def text_to_sequence(text, word2vec_model):\n",
    "    return [\n",
    "        word2vec_model.key_to_index.get(word, 0) for word in mecab.parse(text).split()\n",
    "    ]\n",
    "\n",
    "\n",
    "# データセットクラス\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word2vec_model, max_length):\n",
    "        self.texts = [text_to_sequence(text, word2vec_model) for text in texts]\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # パディングの追加\n",
    "        text = self.texts[idx]\n",
    "        if len(text) < self.max_length:\n",
    "            text += [0] * (self.max_length - len(text))\n",
    "        text = text[: self.max_length]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(\n",
    "            self.labels[idx], dtype=torch.float\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.563 | Train PPL: 1.756\n",
      "\t Val. Loss: 0.515 |  Val. PPL: 1.673\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.321 | Train PPL: 1.378\n",
      "\t Val. Loss: 0.599 |  Val. PPL: 1.821\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.292 | Train PPL: 1.339\n",
      "\t Val. Loss: 0.350 |  Val. PPL: 1.419\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.189 | Train PPL: 1.208\n",
      "\t Val. Loss: 0.213 |  Val. PPL: 1.237\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.200 | Train PPL: 1.222\n",
      "\t Val. Loss: 0.241 |  Val. PPL: 1.272\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.188 | Train PPL: 1.207\n",
      "\t Val. Loss: 0.200 |  Val. PPL: 1.222\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.137 | Train PPL: 1.146\n",
      "\t Val. Loss: 0.232 |  Val. PPL: 1.261\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.167 | Train PPL: 1.182\n",
      "\t Val. Loss: 0.296 |  Val. PPL: 1.345\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.149 | Train PPL: 1.161\n",
      "\t Val. Loss: 0.220 |  Val. PPL: 1.247\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.147 | Train PPL: 1.158\n",
      "\t Val. Loss: 0.220 |  Val. PPL: 1.246\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.114 | Train PPL: 1.121\n",
      "\t Val. Loss: 0.221 |  Val. PPL: 1.248\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.112 | Train PPL: 1.119\n",
      "\t Val. Loss: 0.300 |  Val. PPL: 1.350\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.098 | Train PPL: 1.103\n",
      "\t Val. Loss: 0.258 |  Val. PPL: 1.294\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.088 | Train PPL: 1.092\n",
      "\t Val. Loss: 0.277 |  Val. PPL: 1.320\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.099 | Train PPL: 1.104\n",
      "\t Val. Loss: 0.259 |  Val. PPL: 1.295\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.127 | Train PPL: 1.136\n",
      "\t Val. Loss: 0.255 |  Val. PPL: 1.291\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.089 | Train PPL: 1.093\n",
      "\t Val. Loss: 0.269 |  Val. PPL: 1.309\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.171 | Train PPL: 1.186\n",
      "\t Val. Loss: 0.215 |  Val. PPL: 1.240\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.086 | Train PPL: 1.090\n",
      "\t Val. Loss: 0.224 |  Val. PPL: 1.250\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.081 | Train PPL: 1.084\n",
      "\t Val. Loss: 0.264 |  Val. PPL: 1.303\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.081 | Train PPL: 1.084\n",
      "\t Val. Loss: 0.302 |  Val. PPL: 1.352\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.093 | Train PPL: 1.098\n",
      "\t Val. Loss: 0.283 |  Val. PPL: 1.327\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.081 | Train PPL: 1.084\n",
      "\t Val. Loss: 0.369 |  Val. PPL: 1.447\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.075 | Train PPL: 1.078\n",
      "\t Val. Loss: 0.336 |  Val. PPL: 1.399\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.062 | Train PPL: 1.064\n",
      "\t Val. Loss: 0.309 |  Val. PPL: 1.362\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.084 | Train PPL: 1.088\n",
      "\t Val. Loss: 0.332 |  Val. PPL: 1.394\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.066 | Train PPL: 1.068\n",
      "\t Val. Loss: 0.331 |  Val. PPL: 1.393\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.085 | Train PPL: 1.089\n",
      "\t Val. Loss: 0.312 |  Val. PPL: 1.366\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.069 | Train PPL: 1.071\n",
      "\t Val. Loss: 0.363 |  Val. PPL: 1.437\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.059 | Train PPL: 1.061\n",
      "\t Val. Loss: 0.412 |  Val. PPL: 1.510\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.104 | Train PPL: 1.110\n",
      "\t Val. Loss: 0.330 |  Val. PPL: 1.391\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.090 | Train PPL: 1.094\n",
      "\t Val. Loss: 0.295 |  Val. PPL: 1.344\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.065 | Train PPL: 1.067\n",
      "\t Val. Loss: 0.325 |  Val. PPL: 1.384\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.038 | Train PPL: 1.039\n",
      "\t Val. Loss: 0.427 |  Val. PPL: 1.532\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.061 | Train PPL: 1.063\n",
      "\t Val. Loss: 0.394 |  Val. PPL: 1.482\n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.035 | Train PPL: 1.036\n",
      "\t Val. Loss: 0.423 |  Val. PPL: 1.526\n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.027 | Train PPL: 1.028\n",
      "\t Val. Loss: 0.621 |  Val. PPL: 1.862\n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.063 | Train PPL: 1.065\n",
      "\t Val. Loss: 0.501 |  Val. PPL: 1.650\n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.067 | Train PPL: 1.070\n",
      "\t Val. Loss: 0.514 |  Val. PPL: 1.673\n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.044 | Train PPL: 1.045\n",
      "\t Val. Loss: 0.397 |  Val. PPL: 1.488\n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.036 | Train PPL: 1.036\n",
      "\t Val. Loss: 0.452 |  Val. PPL: 1.571\n",
      "Epoch: 42\n",
      "\tTrain Loss: 0.032 | Train PPL: 1.032\n",
      "\t Val. Loss: 0.522 |  Val. PPL: 1.685\n",
      "Epoch: 43\n",
      "\tTrain Loss: 0.015 | Train PPL: 1.015\n",
      "\t Val. Loss: 0.637 |  Val. PPL: 1.891\n",
      "Epoch: 44\n",
      "\tTrain Loss: 0.051 | Train PPL: 1.053\n",
      "\t Val. Loss: 0.757 |  Val. PPL: 2.132\n",
      "Epoch: 45\n",
      "\tTrain Loss: 0.031 | Train PPL: 1.031\n",
      "\t Val. Loss: 0.603 |  Val. PPL: 1.827\n",
      "Epoch: 46\n",
      "\tTrain Loss: 0.029 | Train PPL: 1.030\n",
      "\t Val. Loss: 0.600 |  Val. PPL: 1.821\n",
      "Epoch: 47\n",
      "\tTrain Loss: 0.036 | Train PPL: 1.036\n",
      "\t Val. Loss: 0.579 |  Val. PPL: 1.785\n",
      "Epoch: 48\n",
      "\tTrain Loss: 0.026 | Train PPL: 1.026\n",
      "\t Val. Loss: 0.582 |  Val. PPL: 1.790\n",
      "Epoch: 49\n",
      "\tTrain Loss: 0.015 | Train PPL: 1.015\n",
      "\t Val. Loss: 0.698 |  Val. PPL: 2.010\n",
      "Epoch: 50\n",
      "\tTrain Loss: 0.003 | Train PPL: 1.003\n",
      "\t Val. Loss: 0.767 |  Val. PPL: 2.153\n",
      "Epoch: 51\n",
      "\tTrain Loss: 0.003 | Train PPL: 1.003\n",
      "\t Val. Loss: 0.819 |  Val. PPL: 2.269\n",
      "Epoch: 52\n",
      "\tTrain Loss: 0.001 | Train PPL: 1.001\n",
      "\t Val. Loss: 0.829 |  Val. PPL: 2.291\n",
      "Epoch: 53\n",
      "\tTrain Loss: 0.001 | Train PPL: 1.001\n",
      "\t Val. Loss: 0.874 |  Val. PPL: 2.396\n",
      "Epoch: 54\n",
      "\tTrain Loss: 0.001 | Train PPL: 1.001\n",
      "\t Val. Loss: 0.895 |  Val. PPL: 2.446\n",
      "Epoch: 55\n",
      "\tTrain Loss: 0.001 | Train PPL: 1.001\n",
      "\t Val. Loss: 0.913 |  Val. PPL: 2.492\n",
      "Epoch: 56\n",
      "\tTrain Loss: 0.001 | Train PPL: 1.001\n",
      "\t Val. Loss: 0.945 |  Val. PPL: 2.574\n",
      "Epoch: 57\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 0.963 |  Val. PPL: 2.619\n",
      "Epoch: 58\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 0.977 |  Val. PPL: 2.657\n",
      "Epoch: 59\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 0.993 |  Val. PPL: 2.700\n",
      "Epoch: 60\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.007 |  Val. PPL: 2.737\n",
      "Epoch: 61\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.028 |  Val. PPL: 2.796\n",
      "Epoch: 62\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.043 |  Val. PPL: 2.839\n",
      "Epoch: 63\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.055 |  Val. PPL: 2.872\n",
      "Epoch: 64\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.068 |  Val. PPL: 2.910\n",
      "Epoch: 65\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.079 |  Val. PPL: 2.942\n",
      "Epoch: 66\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.090 |  Val. PPL: 2.974\n",
      "Epoch: 67\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.105 |  Val. PPL: 3.018\n",
      "Epoch: 68\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.114 |  Val. PPL: 3.047\n",
      "Epoch: 69\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.124 |  Val. PPL: 3.076\n",
      "Epoch: 70\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.135 |  Val. PPL: 3.111\n",
      "Epoch: 71\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.150 |  Val. PPL: 3.157\n",
      "Epoch: 72\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.156 |  Val. PPL: 3.177\n",
      "Epoch: 73\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.165 |  Val. PPL: 3.206\n",
      "Epoch: 74\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.174 |  Val. PPL: 3.234\n",
      "Epoch: 75\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.186 |  Val. PPL: 3.273\n",
      "Epoch: 76\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.195 |  Val. PPL: 3.304\n",
      "Epoch: 77\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.203 |  Val. PPL: 3.330\n",
      "Epoch: 78\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.213 |  Val. PPL: 3.363\n",
      "Epoch: 79\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.222 |  Val. PPL: 3.393\n",
      "Epoch: 80\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.232 |  Val. PPL: 3.427\n",
      "Epoch: 81\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.238 |  Val. PPL: 3.450\n",
      "Epoch: 82\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.246 |  Val. PPL: 3.477\n",
      "Epoch: 83\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.257 |  Val. PPL: 3.513\n",
      "Epoch: 84\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.264 |  Val. PPL: 3.541\n",
      "Epoch: 85\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.274 |  Val. PPL: 3.574\n",
      "Epoch: 86\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.278 |  Val. PPL: 3.589\n",
      "Epoch: 87\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.287 |  Val. PPL: 3.623\n",
      "Epoch: 88\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.296 |  Val. PPL: 3.654\n",
      "Epoch: 89\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.303 |  Val. PPL: 3.682\n",
      "Epoch: 90\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.309 |  Val. PPL: 3.702\n",
      "Epoch: 91\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.318 |  Val. PPL: 3.735\n",
      "Epoch: 92\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.326 |  Val. PPL: 3.765\n",
      "Epoch: 93\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.333 |  Val. PPL: 3.792\n",
      "Epoch: 94\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.341 |  Val. PPL: 3.821\n",
      "Epoch: 95\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.349 |  Val. PPL: 3.854\n",
      "Epoch: 96\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.356 |  Val. PPL: 3.881\n",
      "Epoch: 97\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.362 |  Val. PPL: 3.905\n",
      "Epoch: 98\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.370 |  Val. PPL: 3.935\n",
      "Epoch: 99\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.378 |  Val. PPL: 3.967\n",
      "Epoch: 100\n",
      "\tTrain Loss: 0.000 | Train PPL: 1.000\n",
      "\t Val. Loss: 1.384 |  Val. PPL: 3.993\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../data/train.tsv\", sep=\"\\t\")\n",
    "valid_df = pd.read_csv(\"../data/valid.tsv\", sep=\"\\t\")\n",
    "test_df = pd.read_csv(\"../data/test.tsv\", sep=\"\\t\")\n",
    "\n",
    "# 文の最大長を決定\n",
    "max_length = max(\n",
    "    len(text_to_sequence(text, word2vec_model))\n",
    "    for text in pd.concat([train_df[\"poem\"], valid_df[\"poem\"], test_df[\"poem\"]])\n",
    ")\n",
    "\n",
    "# データセットの分割\n",
    "train_dataset = TextDataset(\n",
    "    train_df[\"poem\"], train_df[\"label\"], word2vec_model, max_length\n",
    ")\n",
    "valid_dataset = TextDataset(\n",
    "    valid_df[\"poem\"], valid_df[\"label\"], word2vec_model, max_length\n",
    ")\n",
    "test_dataset = TextDataset(\n",
    "    test_df[\"poem\"], test_df[\"label\"], word2vec_model, max_length\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2)\n",
    "\n",
    "# モデル、損失関数、最適化手法の設定\n",
    "model = SelfAttentionModel(vocab_size, embedding_dim, hidden_dim, 1, weights)  # 出力は1次元\n",
    "loss_function = nn.BCEWithLogitsLoss()  # 2値分類のための損失関数\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "\n",
    "# 訓練関数\n",
    "def train(model, iterator, optimizer, loss_function):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for text, label in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text).squeeze(1)\n",
    "        loss = loss_function(predictions, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(iterator)\n",
    "\n",
    "\n",
    "# 評価関数\n",
    "def evaluate(model, iterator, loss_function):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, label in iterator:\n",
    "            predictions = model(text).squeeze(1)\n",
    "            loss = loss_function(predictions, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(iterator)\n",
    "\n",
    "\n",
    "# 訓練ループ\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, loss_function)\n",
    "    valid_loss = evaluate(model, valid_loader, loss_function)\n",
    "    print(f\"Epoch: {epoch+1:02}\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}\")\n",
    "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:  # \"_\"はラベルやターゲットを使わない場合\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 予測結果をリストに追加\n",
    "        all_predictions.extend(outputs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = (np.stack(all_predictions).flatten() > 0).astype(float)\n",
    "accuracy_score(test_df[\"label\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_classify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
