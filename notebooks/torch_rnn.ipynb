{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (embedding): Embedding(751361, 100)\n",
      "  (rnn): RNN(100, 32, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "# Word2Vecモデルのロード\n",
    "# gensimで学習済みモデルを読み込む\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"../data/word2vec_ja/jawiki.word_vectors.100d.txt\", binary=False\n",
    ")\n",
    "\n",
    "# 重みを取得\n",
    "weights = torch.FloatTensor(word2vec_model.vectors)\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, weights):\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        # Embedding層の作成\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, _weight=weights)\n",
    "\n",
    "        # Embedding層の重みをfreeze\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        # RNN層の作成\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # 線形層の作成\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        rnn_output, _ = self.rnn(embedded)\n",
    "        output = self.fc(rnn_output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "\n",
    "# パラメータ\n",
    "vocab_size = len(word2vec_model.index_to_key)  # Word2Vecの語彙サイズ\n",
    "embedding_dim = 100\n",
    "hidden_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import MeCab\n",
    "\n",
    "mecab = MeCab.Tagger(\n",
    "    \"-O wakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
    ")\n",
    "\n",
    "\n",
    "# テキストをIDのシーケンスに変換する関数\n",
    "def text_to_sequence(text, word2vec_model):\n",
    "    return [\n",
    "        word2vec_model.key_to_index.get(word, 0) for word in mecab.parse(text).split()\n",
    "    ]\n",
    "\n",
    "\n",
    "# データセットクラス\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word2vec_model, max_length):\n",
    "        self.texts = [text_to_sequence(text, word2vec_model) for text in texts]\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # パディングの追加\n",
    "        text = self.texts[idx]\n",
    "        if len(text) < self.max_length:\n",
    "            text += [0] * (self.max_length - len(text))\n",
    "        text = text[: self.max_length]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(\n",
    "            self.labels[idx], dtype=torch.float\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.598 | Train PPL: 1.818\n",
      "\t Val. Loss: 0.608 |  Val. PPL: 1.837\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.578 | Train PPL: 1.783\n",
      "\t Val. Loss: 0.608 |  Val. PPL: 1.837\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.575 | Train PPL: 1.778\n",
      "\t Val. Loss: 0.621 |  Val. PPL: 1.861\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.577 | Train PPL: 1.780\n",
      "\t Val. Loss: 0.610 |  Val. PPL: 1.840\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.573 | Train PPL: 1.773\n",
      "\t Val. Loss: 0.608 |  Val. PPL: 1.837\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.573 | Train PPL: 1.774\n",
      "\t Val. Loss: 0.614 |  Val. PPL: 1.849\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.573 | Train PPL: 1.774\n",
      "\t Val. Loss: 0.624 |  Val. PPL: 1.867\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.570 | Train PPL: 1.767\n",
      "\t Val. Loss: 0.615 |  Val. PPL: 1.850\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.572 | Train PPL: 1.772\n",
      "\t Val. Loss: 0.608 |  Val. PPL: 1.838\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.571 | Train PPL: 1.770\n",
      "\t Val. Loss: 0.611 |  Val. PPL: 1.842\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.766\n",
      "\t Val. Loss: 0.609 |  Val. PPL: 1.839\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.571 | Train PPL: 1.769\n",
      "\t Val. Loss: 0.615 |  Val. PPL: 1.850\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.765\n",
      "\t Val. Loss: 0.616 |  Val. PPL: 1.851\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.617 |  Val. PPL: 1.854\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.574 | Train PPL: 1.775\n",
      "\t Val. Loss: 0.618 |  Val. PPL: 1.855\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.765\n",
      "\t Val. Loss: 0.610 |  Val. PPL: 1.840\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.570 | Train PPL: 1.768\n",
      "\t Val. Loss: 0.610 |  Val. PPL: 1.840\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.765\n",
      "\t Val. Loss: 0.611 |  Val. PPL: 1.843\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.767\n",
      "\t Val. Loss: 0.609 |  Val. PPL: 1.838\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.613 |  Val. PPL: 1.846\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.844\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.766\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.844\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.762\n",
      "\t Val. Loss: 0.609 |  Val. PPL: 1.838\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.566 | Train PPL: 1.761\n",
      "\t Val. Loss: 0.613 |  Val. PPL: 1.845\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.611 |  Val. PPL: 1.842\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.765\n",
      "\t Val. Loss: 0.620 |  Val. PPL: 1.859\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.765\n",
      "\t Val. Loss: 0.613 |  Val. PPL: 1.846\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.766\n",
      "\t Val. Loss: 0.615 |  Val. PPL: 1.850\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.570 | Train PPL: 1.769\n",
      "\t Val. Loss: 0.613 |  Val. PPL: 1.845\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.615 |  Val. PPL: 1.850\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.570 | Train PPL: 1.769\n",
      "\t Val. Loss: 0.616 |  Val. PPL: 1.852\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.574 | Train PPL: 1.776\n",
      "\t Val. Loss: 0.642 |  Val. PPL: 1.900\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.564 | Train PPL: 1.758\n",
      "\t Val. Loss: 0.660 |  Val. PPL: 1.935\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.583 | Train PPL: 1.791\n",
      "\t Val. Loss: 0.623 |  Val. PPL: 1.864\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.563 | Train PPL: 1.755\n",
      "\t Val. Loss: 0.646 |  Val. PPL: 1.908\n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.583 | Train PPL: 1.791\n",
      "\t Val. Loss: 0.634 |  Val. PPL: 1.885\n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.576 | Train PPL: 1.779\n",
      "\t Val. Loss: 0.616 |  Val. PPL: 1.852\n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.566 | Train PPL: 1.761\n",
      "\t Val. Loss: 0.600 |  Val. PPL: 1.823\n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.574 | Train PPL: 1.775\n",
      "\t Val. Loss: 0.609 |  Val. PPL: 1.838\n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.570 | Train PPL: 1.768\n",
      "\t Val. Loss: 0.620 |  Val. PPL: 1.859\n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.620 |  Val. PPL: 1.859\n",
      "Epoch: 42\n",
      "\tTrain Loss: 0.557 | Train PPL: 1.746\n",
      "\t Val. Loss: 0.635 |  Val. PPL: 1.887\n",
      "Epoch: 43\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.762\n",
      "\t Val. Loss: 0.619 |  Val. PPL: 1.856\n",
      "Epoch: 44\n",
      "\tTrain Loss: 0.572 | Train PPL: 1.772\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.844\n",
      "Epoch: 45\n",
      "\tTrain Loss: 0.572 | Train PPL: 1.771\n",
      "\t Val. Loss: 0.611 |  Val. PPL: 1.843\n",
      "Epoch: 46\n",
      "\tTrain Loss: 0.570 | Train PPL: 1.767\n",
      "\t Val. Loss: 0.611 |  Val. PPL: 1.843\n",
      "Epoch: 47\n",
      "\tTrain Loss: 0.575 | Train PPL: 1.777\n",
      "\t Val. Loss: 0.610 |  Val. PPL: 1.840\n",
      "Epoch: 48\n",
      "\tTrain Loss: 0.571 | Train PPL: 1.770\n",
      "\t Val. Loss: 0.609 |  Val. PPL: 1.838\n",
      "Epoch: 49\n",
      "\tTrain Loss: 0.570 | Train PPL: 1.769\n",
      "\t Val. Loss: 0.617 |  Val. PPL: 1.853\n",
      "Epoch: 50\n",
      "\tTrain Loss: 0.570 | Train PPL: 1.768\n",
      "\t Val. Loss: 0.608 |  Val. PPL: 1.838\n",
      "Epoch: 51\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.765\n",
      "\t Val. Loss: 0.611 |  Val. PPL: 1.841\n",
      "Epoch: 52\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.765\n",
      "\t Val. Loss: 0.609 |  Val. PPL: 1.838\n",
      "Epoch: 53\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.767\n",
      "\t Val. Loss: 0.608 |  Val. PPL: 1.838\n",
      "Epoch: 54\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.766\n",
      "\t Val. Loss: 0.608 |  Val. PPL: 1.837\n",
      "Epoch: 55\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.614 |  Val. PPL: 1.847\n",
      "Epoch: 56\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.763\n",
      "\t Val. Loss: 0.610 |  Val. PPL: 1.840\n",
      "Epoch: 57\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.763\n",
      "\t Val. Loss: 0.610 |  Val. PPL: 1.840\n",
      "Epoch: 58\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.766\n",
      "\t Val. Loss: 0.611 |  Val. PPL: 1.843\n",
      "Epoch: 59\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.766\n",
      "\t Val. Loss: 0.608 |  Val. PPL: 1.837\n",
      "Epoch: 60\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.763\n",
      "\t Val. Loss: 0.611 |  Val. PPL: 1.843\n",
      "Epoch: 61\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.763\n",
      "\t Val. Loss: 0.613 |  Val. PPL: 1.845\n",
      "Epoch: 62\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.766\n",
      "\t Val. Loss: 0.613 |  Val. PPL: 1.846\n",
      "Epoch: 63\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.844\n",
      "Epoch: 64\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.762\n",
      "\t Val. Loss: 0.609 |  Val. PPL: 1.839\n",
      "Epoch: 65\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.765\n",
      "\t Val. Loss: 0.610 |  Val. PPL: 1.840\n",
      "Epoch: 66\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.767\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.845\n",
      "Epoch: 67\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.763\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.844\n",
      "Epoch: 68\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.845\n",
      "Epoch: 69\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.766\n",
      "\t Val. Loss: 0.611 |  Val. PPL: 1.843\n",
      "Epoch: 70\n",
      "\tTrain Loss: 0.571 | Train PPL: 1.770\n",
      "\t Val. Loss: 0.618 |  Val. PPL: 1.856\n",
      "Epoch: 71\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.614 |  Val. PPL: 1.848\n",
      "Epoch: 72\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.844\n",
      "Epoch: 73\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.610 |  Val. PPL: 1.841\n",
      "Epoch: 74\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.767\n",
      "\t Val. Loss: 0.608 |  Val. PPL: 1.836\n",
      "Epoch: 75\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.766\n",
      "\t Val. Loss: 0.613 |  Val. PPL: 1.847\n",
      "Epoch: 76\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.763\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.844\n",
      "Epoch: 77\n",
      "\tTrain Loss: 0.571 | Train PPL: 1.769\n",
      "\t Val. Loss: 0.611 |  Val. PPL: 1.842\n",
      "Epoch: 78\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.611 |  Val. PPL: 1.842\n",
      "Epoch: 79\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.762\n",
      "\t Val. Loss: 0.613 |  Val. PPL: 1.846\n",
      "Epoch: 80\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.765\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.845\n",
      "Epoch: 81\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.763\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.844\n",
      "Epoch: 82\n",
      "\tTrain Loss: 0.566 | Train PPL: 1.762\n",
      "\t Val. Loss: 0.611 |  Val. PPL: 1.842\n",
      "Epoch: 83\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.765\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.844\n",
      "Epoch: 84\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.615 |  Val. PPL: 1.849\n",
      "Epoch: 85\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.766\n",
      "\t Val. Loss: 0.609 |  Val. PPL: 1.839\n",
      "Epoch: 86\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.766\n",
      "\t Val. Loss: 0.611 |  Val. PPL: 1.843\n",
      "Epoch: 87\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.762\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.844\n",
      "Epoch: 88\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.609 |  Val. PPL: 1.839\n",
      "Epoch: 89\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.613 |  Val. PPL: 1.846\n",
      "Epoch: 90\n",
      "\tTrain Loss: 0.566 | Train PPL: 1.762\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.845\n",
      "Epoch: 91\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.765\n",
      "\t Val. Loss: 0.611 |  Val. PPL: 1.842\n",
      "Epoch: 92\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.601 |  Val. PPL: 1.824\n",
      "Epoch: 93\n",
      "\tTrain Loss: 0.578 | Train PPL: 1.783\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.844\n",
      "Epoch: 94\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.763\n",
      "\t Val. Loss: 0.613 |  Val. PPL: 1.846\n",
      "Epoch: 95\n",
      "\tTrain Loss: 0.569 | Train PPL: 1.766\n",
      "\t Val. Loss: 0.613 |  Val. PPL: 1.846\n",
      "Epoch: 96\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.615 |  Val. PPL: 1.850\n",
      "Epoch: 97\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.765\n",
      "\t Val. Loss: 0.613 |  Val. PPL: 1.846\n",
      "Epoch: 98\n",
      "\tTrain Loss: 0.568 | Train PPL: 1.765\n",
      "\t Val. Loss: 0.613 |  Val. PPL: 1.846\n",
      "Epoch: 99\n",
      "\tTrain Loss: 0.570 | Train PPL: 1.769\n",
      "\t Val. Loss: 0.614 |  Val. PPL: 1.848\n",
      "Epoch: 100\n",
      "\tTrain Loss: 0.567 | Train PPL: 1.764\n",
      "\t Val. Loss: 0.612 |  Val. PPL: 1.844\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../data/train.tsv\", sep=\"\\t\")\n",
    "valid_df = pd.read_csv(\"../data/valid.tsv\", sep=\"\\t\")\n",
    "test_df = pd.read_csv(\"../data/test.tsv\", sep=\"\\t\")\n",
    "\n",
    "# 文の最大長を決定\n",
    "max_length = max(\n",
    "    len(text_to_sequence(text, word2vec_model))\n",
    "    for text in pd.concat([train_df[\"poem\"], valid_df[\"poem\"], test_df[\"poem\"]])\n",
    ")\n",
    "\n",
    "# データセットの分割\n",
    "train_dataset = TextDataset(\n",
    "    train_df[\"poem\"], train_df[\"label\"], word2vec_model, max_length\n",
    ")\n",
    "valid_dataset = TextDataset(\n",
    "    valid_df[\"poem\"], valid_df[\"label\"], word2vec_model, max_length\n",
    ")\n",
    "test_dataset = TextDataset(\n",
    "    test_df[\"poem\"], test_df[\"label\"], word2vec_model, max_length\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2)\n",
    "\n",
    "# モデル、損失関数、最適化手法の設定\n",
    "model = RNNModel(vocab_size, embedding_dim, hidden_dim, 1, weights)  # 出力は1次元\n",
    "loss_function = nn.BCEWithLogitsLoss()  # 2値分類のための損失関数\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "\n",
    "# 訓練関数\n",
    "def train(model, iterator, optimizer, loss_function):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for text, label in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text).squeeze(1)\n",
    "        loss = loss_function(predictions, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(iterator)\n",
    "\n",
    "\n",
    "# 評価関数\n",
    "def evaluate(model, iterator, loss_function):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, label in iterator:\n",
    "            predictions = model(text).squeeze(1)\n",
    "            loss = loss_function(predictions, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(iterator)\n",
    "\n",
    "\n",
    "# 訓練ループ\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, loss_function)\n",
    "    valid_loss = evaluate(model, valid_loader, loss_function)\n",
    "    print(f\"Epoch: {epoch+1:02}\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}\")\n",
    "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:  # \"_\"はラベルやターゲットを使わない場合\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 予測結果をリストに追加\n",
    "        all_predictions.extend(outputs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.3887568], dtype=float32),\n",
       " array([1.4108727], dtype=float32),\n",
       " array([1.3927056], dtype=float32),\n",
       " array([1.8779539], dtype=float32),\n",
       " array([1.7121079], dtype=float32),\n",
       " array([1.3825288], dtype=float32),\n",
       " array([1.3263783], dtype=float32),\n",
       " array([1.4227681], dtype=float32),\n",
       " array([1.3441978], dtype=float32),\n",
       " array([2.0596204], dtype=float32),\n",
       " array([1.8088042], dtype=float32),\n",
       " array([1.4110069], dtype=float32),\n",
       " array([1.2077798], dtype=float32),\n",
       " array([1.4284291], dtype=float32),\n",
       " array([1.5877194], dtype=float32),\n",
       " array([1.3353667], dtype=float32),\n",
       " array([1.0406523], dtype=float32),\n",
       " array([1.4238864], dtype=float32),\n",
       " array([2.2286155], dtype=float32),\n",
       " array([1.5552645], dtype=float32),\n",
       " array([0.8097642], dtype=float32),\n",
       " array([0.97168744], dtype=float32),\n",
       " array([0.57724035], dtype=float32),\n",
       " array([1.3819319], dtype=float32),\n",
       " array([1.4538323], dtype=float32),\n",
       " array([1.7465686], dtype=float32),\n",
       " array([1.3662088], dtype=float32),\n",
       " array([2.2249575], dtype=float32),\n",
       " array([2.3278866], dtype=float32),\n",
       " array([2.3314116], dtype=float32),\n",
       " array([1.1766325], dtype=float32),\n",
       " array([1.4229388], dtype=float32),\n",
       " array([1.162313], dtype=float32),\n",
       " array([0.59711707], dtype=float32),\n",
       " array([1.4260044], dtype=float32),\n",
       " array([1.4165144], dtype=float32),\n",
       " array([0.7585835], dtype=float32),\n",
       " array([1.4123079], dtype=float32),\n",
       " array([1.754853], dtype=float32),\n",
       " array([1.1767775], dtype=float32),\n",
       " array([1.4367086], dtype=float32),\n",
       " array([1.4127939], dtype=float32),\n",
       " array([1.3263355], dtype=float32),\n",
       " array([1.4360689], dtype=float32),\n",
       " array([1.2733806], dtype=float32),\n",
       " array([1.4790299], dtype=float32),\n",
       " array([1.4056193], dtype=float32),\n",
       " array([2.1001947], dtype=float32),\n",
       " array([1.3956226], dtype=float32),\n",
       " array([1.2650038], dtype=float32),\n",
       " array([1.2655116], dtype=float32),\n",
       " array([1.1394364], dtype=float32),\n",
       " array([2.0503561], dtype=float32),\n",
       " array([0.6756867], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_classify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
